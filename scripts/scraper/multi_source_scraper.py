"""
Enhanced Multi-Source Department 56 Scraper

Scrapes from multiple sources to improve data confidence:
1. retiredproducts.department56.com (current working scraper)
2. department56.com (official current products)  
3. replacements.com (cross-reference and relationships)

Features:
- Cross-source data validation
- Enhanced confidence scoring based on multiple confirmations
- Automatic series/collection discovery
- House-accessory relationship detection
- Different scraping strategies for houses vs accessories
"""

import os
import time
import json
import asyncio
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Union
from urllib.parse import quote_plus, urljoin
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor

# Third-party imports
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    from bs4 import BeautifulSoup
    from rapidfuzz import fuzz
    from supabase import create_client, Client
    from dotenv import load_dotenv
except ImportError as e:
    print(f"Missing required package: {e}")
    print("\nInstall required packages with:")
    print("pip install -r requirements.txt")
    exit(1)

# Load environment variables
load_dotenv()

# Supabase configuration
SUPABASE_URL = os.getenv("VITE_SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
    print("ERROR: Missing Supabase credentials in .env file")
    print("Required: VITE_SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY")
    exit(1)

# Initialize Supabase client
supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

@dataclass
class ScrapedItem:
    """Data structure for scraped item information"""
    name: str
    item_number: str = ""
    intro_year: Optional[int] = None
    retire_year: Optional[int] = None
    description: str = ""
    dimensions: str = ""
    primary_image_url: str = ""
    additional_images: List[str] = field(default_factory=list)
    source_url: str = ""
    source_site: str = ""
    discovered_series: str = ""
    discovered_collection: str = ""
    item_type: str = "house"  # "house" or "accessory"
    related_items: List[str] = field(default_factory=list)
    confidence_details: Dict = field(default_factory=dict)

@dataclass 
class MultiSourceResult:
    """Aggregated results from multiple sources"""
    best_match: ScrapedItem
    all_sources: Dict[str, ScrapedItem]
    overall_confidence: float
    name_match_score: float
    details_confidence: float
    cross_source_validation: float

class MultiSourceScraper:
    """Enhanced scraper that searches multiple Department 56 sources"""
    
    def __init__(self, rate_limit_delay: float = 2.0):
        self.rate_limit_delay = rate_limit_delay
        self.session = self._create_session()
        
        # Site configurations
        self.sites = {
            'dept56_retired': {
                'base_url': 'https://retiredproducts.department56.com',
                'search_url': 'https://retiredproducts.department56.com/search?q={}',
                'enabled': True
            },
            'dept56_official': {
                'base_url': 'https://department56.com',
                'search_url': 'https://department56.com/search?q={}',
                'enabled': True
            },
            'replacements': {
                'base_url': 'https://www.replacements.com',
                'search_url': 'https://www.replacements.com/search?query={}',
                'enabled': True
            }
        }
        
    def _create_session(self) -> requests.Session:
        """Create session with retry strategy and headers"""
        session = requests.Session()
        
        # Retry strategy
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # Headers to appear like a regular browser
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        return session
    
    async def search_all_sources(self, item_name: str, item_number: str = None, item_type: str = "house") -> MultiSourceResult:
        """
        Search item across all enabled sources and aggregate results
        
        Args:
            item_name: Name to search for
            item_number: Optional SKU/item number
            item_type: "house" or "accessory" (affects scraping strategy)
            
        Returns:
            MultiSourceResult with best match and confidence scores
        """
        print(f"\nüîç Starting multi-source search for: {item_name}")
        if item_number:
            print(f"üìã Item number: {item_number}")
        print(f"üè† Item type: {item_type}")
        
        # Search all sources concurrently
        tasks = []
        for source_name, config in self.sites.items():
            if config['enabled']:
                if source_name == 'dept56_retired':
                    tasks.append(self._search_dept56_retired(item_name, item_number))
                elif source_name == 'dept56_official':
                    tasks.append(self._search_dept56_official(item_name, item_number))
                elif source_name == 'replacements':
                    tasks.append(self._search_replacements(item_name, item_number))
        
        # Execute searches with rate limiting
        results = {}
        for i, task in enumerate(tasks):
            if i > 0:  # Don't delay before first request
                time.sleep(self.rate_limit_delay)
            
            try:
                source_name = list(self.sites.keys())[i]
                result = await task
                if result:
                    results[source_name] = result
                    print(f"‚úÖ {source_name}: Found match")
                else:
                    print(f"‚ùå {source_name}: No match found")
            except Exception as e:
                source_name = list(self.sites.keys())[i]
                print(f"‚ö†Ô∏è {source_name}: Error - {str(e)}")
        
        # Aggregate and score results
        return self._aggregate_results(item_name, results, item_type)
    
    async def _search_dept56_retired(self, item_name: str, item_number: str = None) -> Optional[ScrapedItem]:
        """Search retiredproducts.department56.com (existing working scraper)"""
        search_query = item_number if item_number else item_name
        search_url = self.sites['dept56_retired']['search_url'].format(quote_plus(search_query))
        
        try:
            response = self.session.get(search_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Find product results (adapt to actual site structure)
            products = soup.find_all('div', class_='productgrid--item') or soup.find_all('div', class_='product-item')
            
            if not products:
                return None
            
            # Find best match using fuzzy matching
            best_match = None
            best_score = 0
            
            for product in products[:5]:  # Check first 5 results
                title_elem = product.find('h3') or product.find('h4') or product.find('a', class_='title')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                score = fuzz.token_sort_ratio(search_query.lower(), title.lower())
                
                if score > best_score and score >= 60:  # 60% minimum match
                    best_score = score
                    
                    # Extract item details
                    item_link = title_elem.find('a')['href'] if title_elem.find('a') else None
                    if item_link and not item_link.startswith('http'):
                        item_link = urljoin(self.sites['dept56_retired']['base_url'], item_link)
                    
                    best_match = ScrapedItem(
                        name=title,
                        source_url=item_link or search_url,
                        source_site='dept56_retired'
                    )
            
            if best_match and best_match.source_url != search_url:
                # Get detailed information from product page
                await self._extract_dept56_retired_details(best_match)
            
            return best_match
            
        except Exception as e:
            print(f"Error searching dept56_retired: {e}")
            return None
    
    async def _search_dept56_official(self, item_name: str, item_number: str = None) -> Optional[ScrapedItem]:
        """Search department56.com for current products"""
        search_query = item_number if item_number else item_name
        search_url = self.sites['dept56_official']['search_url'].format(quote_plus(search_query))
        
        try:
            response = self.session.get(search_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Adapt selectors to actual Department 56 site structure
            products = soup.find_all('div', class_='product-tile') or soup.find_all('article', class_='product')
            
            if not products:
                return None
            
            best_match = None
            best_score = 0
            
            for product in products[:5]:
                title_elem = product.find('h3') or product.find('h2') or product.find('a', class_='product-title')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                score = fuzz.token_sort_ratio(search_query.lower(), title.lower())
                
                if score > best_score and score >= 60:
                    best_score = score
                    
                    item_link = title_elem.find('a')['href'] if title_elem.find('a') else None
                    if item_link and not item_link.startswith('http'):
                        item_link = urljoin(self.sites['dept56_official']['base_url'], item_link)
                    
                    best_match = ScrapedItem(
                        name=title,
                        source_url=item_link or search_url,
                        source_site='dept56_official'
                    )
            
            if best_match and best_match.source_url != search_url:
                await self._extract_dept56_official_details(best_match)
            
            return best_match
            
        except Exception as e:
            print(f"Error searching dept56_official: {e}")
            return None
    
    async def _search_replacements(self, item_name: str, item_number: str = None) -> Optional[ScrapedItem]:
        """Search replacements.com for cross-reference data"""
        search_query = f"Department 56 {item_number or item_name}"
        search_url = self.sites['replacements']['search_url'].format(quote_plus(search_query))
        
        try:
            response = self.session.get(search_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Adapt to Replacements.com structure
            products = soup.find_all('div', class_='item') or soup.find_all('tr', class_='product-row')
            
            if not products:
                return None
            
            best_match = None
            best_score = 0
            
            for product in products[:5]:
                title_elem = product.find('a', class_='item-title') or product.find('h3')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                
                # Filter for Department 56 items
                if 'department 56' not in title.lower():
                    continue
                    
                score = fuzz.token_sort_ratio(search_query.lower(), title.lower())
                
                if score > best_score and score >= 50:  # Lower threshold for cross-reference
                    best_score = score
                    
                    item_link = title_elem['href'] if title_elem.get('href') else None
                    if item_link and not item_link.startswith('http'):
                        item_link = urljoin(self.sites['replacements']['base_url'], item_link)
                    
                    best_match = ScrapedItem(
                        name=title,
                        source_url=item_link or search_url,
                        source_site='replacements'
                    )
            
            if best_match and best_match.source_url != search_url:
                await self._extract_replacements_details(best_match)
            
            return best_match
            
        except Exception as e:
            print(f"Error searching replacements: {e}")
            return None
    
    async def _extract_dept56_retired_details(self, item: ScrapedItem):
        """Extract detailed info from retired products page"""
        try:
            response = self.session.get(item.source_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract SKU
            sku_elem = soup.find('span', string=lambda text: text and 'SKU' in text)
            if sku_elem:
                item.item_number = sku_elem.get_text().replace('SKU:', '').strip()
            
            # Extract description
            desc_elem = soup.find('div', class_='description') or soup.find('div', class_='product-description')
            if desc_elem:
                item.description = desc_elem.get_text(strip=True)
            
            # Extract years
            year_text = soup.get_text()
            import re
            year_matches = re.findall(r'(\d{4})', year_text)
            if year_matches:
                # Look for intro/retire year patterns
                for year in year_matches:
                    year_int = int(year)
                    if 1976 <= year_int <= datetime.now().year:  # Department 56 started in 1976
                        if not item.intro_year:
                            item.intro_year = year_int
                        elif year_int > item.intro_year:
                            item.retire_year = year_int
            
            # Extract series/collection from breadcrumbs or categories
            breadcrumbs = soup.find('nav', class_='breadcrumb') or soup.find('div', class_='breadcrumbs')
            if breadcrumbs:
                links = breadcrumbs.find_all('a')
                for link in links:
                    text = link.get_text(strip=True)
                    if 'village' in text.lower() or 'series' in text.lower():
                        item.discovered_series = text
                    elif 'collection' in text.lower():
                        item.discovered_collection = text
            
            # Extract images
            img_elems = soup.find_all('img', src=True)
            for img in img_elems:
                src = img['src']
                if src and 'product' in src.lower():
                    if not item.primary_image_url:
                        item.primary_image_url = urljoin(item.source_url, src)
                    else:
                        item.additional_images.append(urljoin(item.source_url, src))
                        
        except Exception as e:
            print(f"Error extracting details from {item.source_url}: {e}")
    
    async def _extract_dept56_official_details(self, item: ScrapedItem):
        """Extract details from official Department 56 site"""
        # Similar structure to retired site extraction
        # Implementation would be adapted to the current site's HTML structure
        pass
    
    async def _extract_replacements_details(self, item: ScrapedItem):
        """Extract details and relationships from Replacements.com"""
        try:
            response = self.session.get(item.source_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for "goes with" or "related items" sections
            related_section = soup.find('div', string=lambda text: text and 'goes with' in text.lower())
            if related_section:
                related_items = related_section.find_next_siblings('a')
                item.related_items = [a.get_text(strip=True) for a in related_items[:5]]
            
            # Extract item number if available
            item_num_elem = soup.find('span', string=lambda text: text and any(x in text for x in ['#', 'SKU', 'Item']))
            if item_num_elem:
                item.item_number = item_num_elem.get_text().strip()
                
        except Exception as e:
            print(f"Error extracting Replacements details: {e}")
    
    def _aggregate_results(self, search_name: str, results: Dict[str, ScrapedItem], item_type: str) -> MultiSourceResult:
        """Aggregate results from multiple sources and calculate confidence"""
        if not results:
            return MultiSourceResult(
                best_match=ScrapedItem(name=search_name, item_type=item_type),
                all_sources={},
                overall_confidence=0.0,
                name_match_score=0.0,
                details_confidence=0.0,
                cross_source_validation=0.0
            )
        
        # Find best match by name similarity
        best_match = None
        best_name_score = 0
        
        for source, item in results.items():
            name_score = fuzz.token_sort_ratio(search_name.lower(), item.name.lower()) / 100
            if name_score > best_name_score:
                best_name_score = name_score
                best_match = item
        
        # Calculate cross-source validation
        cross_source_score = 0.0
        if len(results) > 1:
            # Check if multiple sources agree on key details
            names = [item.name for item in results.values()]
            item_numbers = [item.item_number for item in results.values() if item.item_number]
            
            # Name consistency across sources
            name_agreement = sum(fuzz.ratio(names[0], name) > 80 for name in names[1:]) / max(len(names) - 1, 1)
            
            # Item number consistency
            number_agreement = 1.0 if len(set(item_numbers)) <= 1 and item_numbers else 0.5
            
            cross_source_score = (name_agreement + number_agreement) / 2
        
        # Calculate details confidence
        details_score = 0.0
        if best_match:
            required_fields = ['item_number', 'description', 'intro_year', 'primary_image_url']
            available_fields = sum(1 for field in required_fields if getattr(best_match, field))
            details_score = available_fields / len(required_fields)
            
            # Bonus for series/collection discovery
            if best_match.discovered_series or best_match.discovered_collection:
                details_score += 0.1
            
            # Bonus for multiple images
            if len(best_match.additional_images) > 0:
                details_score += 0.05
        
        # Overall confidence calculation
        overall_confidence = (
            best_name_score * 0.5 +
            details_score * 0.3 +
            cross_source_score * 0.2
        )
        
        print(f"üìä Confidence Scores:")
        print(f"   Name Match: {best_name_score:.2f}")
        print(f"   Details: {details_score:.2f}")
        print(f"   Cross-Source: {cross_source_score:.2f}")
        print(f"   Overall: {overall_confidence:.2f}")
        
        return MultiSourceResult(
            best_match=best_match or ScrapedItem(name=search_name, item_type=item_type),
            all_sources=results,
            overall_confidence=min(overall_confidence, 1.0),
            name_match_score=best_name_score,
            details_confidence=details_score,
            cross_source_validation=cross_source_score
        )
    
    async def save_to_staging(self, result: MultiSourceResult, original_house_id: str = None, original_accessory_id: str = None):
        """Save aggregated results to staging table"""
        item = result.best_match
        
        # Determine which staging table to use
        if item.item_type == "accessory":
            table_name = "staged_accessories"
            original_id_field = "original_accessory_id"
            original_id_value = original_accessory_id
        else:
            table_name = "staged_houses"
            original_id_field = "original_house_id"
            original_id_value = original_house_id
        
        # Prepare data for insertion
        staging_data = {
            original_id_field: original_id_value,
            'item_number': item.item_number,
            'name': item.name,
            'intro_year': item.intro_year,
            'retire_year': item.retire_year,
            'description': item.description,
            'dimensions': item.dimensions,
            'primary_image_url': item.primary_image_url,
            'additional_images': item.additional_images,
            'discovered_series': item.discovered_series,
            'discovered_collection': item.discovered_collection,
            'name_match_score': result.name_match_score,
            'details_confidence_score': result.details_confidence,
            'overall_confidence_score': result.overall_confidence,
            'status': 'pending'
        }
        
        # Add source URLs
        for source, source_item in result.all_sources.items():
            if source == 'dept56_retired':
                staging_data['dept56_retired_url'] = source_item.source_url
            elif source == 'dept56_official':
                staging_data['dept56_official_url'] = source_item.source_url
            elif source == 'replacements':
                staging_data['replacements_url'] = source_item.source_url
        
        try:
            response = supabase.table(table_name).insert(staging_data).execute()
            print(f"‚úÖ Saved to {table_name}: {item.name}")
            return response.data[0] if response.data else None
        except Exception as e:
            print(f"‚ùå Error saving to {table_name}: {e}")
            return None


# Example usage and testing
async def test_multi_source_scraper():
    """Test the multi-source scraper with sample items"""
    scraper = MultiSourceScraper()
    
    # Test items - mix of houses and accessories
    test_items = [
        {"name": "Robbie's Robot Factory", "item_number": "56.54305", "type": "house"},
        {"name": "Santa's Wonderland House", "item_number": "", "type": "house"},
        {"name": "Village Animated Neon Sign", "item_number": "", "type": "accessory"},
    ]
    
    for item in test_items:
        print(f"\n{'='*60}")
        print(f"Testing: {item['name']}")
        print(f"{'='*60}")
        
        result = await scraper.search_all_sources(
            item['name'], 
            item['item_number'], 
            item['type']
        )
        
        if result.overall_confidence > 0.0:
            print(f"\nüìä FINAL RESULT:")
            print(f"   Best Match: {result.best_match.name}")
            print(f"   Sources Found: {len(result.all_sources)}")
            print(f"   Overall Confidence: {result.overall_confidence:.2f}")
            
            # Save to staging (would need actual IDs for real use)
            await scraper.save_to_staging(result)
        else:
            print(f"\n‚ùå No matches found for {item['name']}")

if __name__ == "__main__":
    # Run test
    asyncio.run(test_multi_source_scraper())